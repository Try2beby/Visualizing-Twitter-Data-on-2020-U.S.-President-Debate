{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original json file like\n",
    "# {\"name\": \"John\", \"age\": 31, \"city\": \"New York\"}\n",
    "# {\"name\": \"John\", \"age\": 31, \"city\": \"New York\"}\n",
    "# want to transform to standard json file like\n",
    "# [{\"name\": \"John\", \"age\": 31, \"city\": \"New York\"},{\"name\": \"John\", \"age\": 31, \"city\": \"New York\"}]\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def processjson(filename, outpath=\"../usedata/\"):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    # Parse each line as a JSON object and add to a list\n",
    "    data = [json.loads(line) for line in lines]\n",
    "    print(len(data))\n",
    "    # if outpath not exist, create it\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "    # Write the list of JSON objects to a new file\n",
    "    with open(outpath + filename, \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "datapath = \"./data/\"\n",
    "dataname = \"2020-10-2%d.json\"\n",
    "day = range(6)\n",
    "\n",
    "for i in day:\n",
    "    filename = dataname % i\n",
    "    # processjson(datapath + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72875\n",
      "Index(['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone',\n",
      "       'user_id', 'username', 'name', 'place', 'tweet', 'language', 'mentions',\n",
      "       'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count',\n",
      "       'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video',\n",
      "       'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
      "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
      "       'trans_dest'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "usedatapath = \"./usedata/\"\n",
    "# load data\n",
    "with open(usedatapath + dataname % 0, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "print(len(data))\n",
    "\n",
    "# transform data to pandas df\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data[:1000])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 72875\n",
      "conversation_id 60365\n",
      "created_at 47102\n",
      "date 2\n",
      "time 47102\n",
      "timezone 1\n",
      "user_id 42304\n",
      "username 42304\n",
      "name 40643\n",
      "place TypeError\n",
      "tweet 70080\n",
      "language 54\n",
      "mentions TypeError\n",
      "urls TypeError\n",
      "photos TypeError\n",
      "replies_count 106\n",
      "retweets_count 271\n",
      "likes_count 442\n",
      "hashtags TypeError\n",
      "cashtags TypeError\n",
      "link 72875\n",
      "retweet 1\n",
      "quote_url 9408\n",
      "video 2\n",
      "thumbnail 15339\n",
      "near 1\n",
      "geo 1\n",
      "source 1\n",
      "user_rt_id 1\n",
      "user_rt 1\n",
      "retweet_id 1\n",
      "reply_to TypeError\n",
      "retweet_date 1\n",
      "translate 1\n",
      "trans_src 1\n",
      "trans_dest 1\n"
     ]
    }
   ],
   "source": [
    "# print number of unique values in each column\n",
    "idx = []\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        print(col, df[col].nunique())\n",
    "        if df[col].nunique() == 0:\n",
    "            idx.append(col)\n",
    "    except TypeError:\n",
    "        print(col, \"TypeError\")\n",
    "\n",
    "# drop columns with no unique values\n",
    "df.drop(idx, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要信息: `id`,`time`,`user_id`,`username`,`tweet`,`language`\n",
    "可用以评价 tweet 影响力的信息：`replies_cout`,`retweets_count`,`likes_count`\n",
    "可判断 tweet 话题的信息：`hashtags`,`mentions`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "en     804\n",
       "und    144\n",
       "es      22\n",
       "fr      13\n",
       "pt       7\n",
       "tl       4\n",
       "fa       2\n",
       "de       2\n",
       "in       1\n",
       "it       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要语言为英语，为方便处理，将其他语言的 tweet 翻译为英语，只保留出现频次大于 100 的语言\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "en     58020\n",
      "und     9983\n",
      "es      1055\n",
      "fr       897\n",
      "de       673\n",
      "it       379\n",
      "pt       253\n",
      "tl       223\n",
      "fa       203\n",
      "nl       192\n",
      "hi       161\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# keep rows with frequency of corresponding language > 100\n",
    "language = df[\"language\"].value_counts() >= 100\n",
    "language = language[language].index.tolist()\n",
    "df = df[df[\"language\"].isin(language)]\n",
    "print(df[\"language\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate all non-en tweets to english\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "def translate(language, tweet):\n",
    "    if language != \"en\":\n",
    "        try:\n",
    "            tweet = translator.translate(tweet).text\n",
    "        except:\n",
    "            tweet = \"\"\n",
    "    return tweet\n",
    "\n",
    "\n",
    "# df[\"tweet\"] = df.apply(lambda x: translate(x[\"language\"], x[\"tweet\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(804, 35)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep rows with en tweets\n",
    "df_en = df[df[\"language\"] == \"en\"].copy()\n",
    "df_en.drop([\"language\"], axis=1, inplace=True)\n",
    "df_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets\n",
    "import re\n",
    "def clean_tweets(text):\n",
    "  text = re.sub(\"RT @[\\w]*:\",\"\",text)\n",
    "  text = re.sub(\"@[\\w]*\",\"\",text)\n",
    "  text = re.sub(\"https?://[A-Za-z0-9./]*\",\"\",text)\n",
    "  text = re.sub(\"\\n\",\"\",text)\n",
    "  return text\n",
    "\n",
    "df_en[\"tweet\"] = df_en[\"tweet\"].apply(lambda x: clean_tweets(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "2    297\n",
       "1    263\n",
       "3    144\n",
       "0    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a new tag column to whether the tweet have string \"biden\" or \"trump\"\n",
    "# 0 for none, 1 for biden, 2 for trump, 3 for both\n",
    "def add_tag(text):\n",
    "  if \"biden\" in text.lower():\n",
    "    if \"trump\" in text.lower():\n",
    "      return 3\n",
    "    else:\n",
    "      return 1\n",
    "  elif \"trump\" in text.lower():\n",
    "    return 2\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "df_en[\"tag\"] = df_en[\"tweet\"].apply(lambda x: add_tag(x))\n",
    "df_en[\"tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the vader_lexicon\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# Create a function to get the polarity scores of each tweet\n",
    "def get_polarity_scores(tweet):\n",
    "    return sia.polarity_scores(tweet)\n",
    "\n",
    "\n",
    "# add polarity scores to df\n",
    "df_en[\"polarity_scores\"] = df_en[\"tweet\"].apply(get_polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a word cloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_wordcloud(tag_list):\n",
    "    # select tweets with tag in tag_list\n",
    "    text = \" \".join(df_en[df_en[\"tag\"].isin(tag_list)][\"tweet\"])\n",
    "    # Create a wordcloud object\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        background_color=\"white\",\n",
    "        stopwords=stopwords,\n",
    "        min_font_size=10,\n",
    "    ).generate(text)\n",
    "    \n",
    "    # plot the wordcloud object\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    # turn off the axis\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.imshow(wordcloud)\n",
    "\n",
    "plot_wordcloud([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a network from the tweets\n",
    "\n",
    "import igraph as ig\n",
    "\n",
    "# G = ig.Graph.TupleList(tuples.itertuples(index=False), \n",
    "#                            directed=True, \n",
    "#                            weights=False,\n",
    "#                            edge_attrs=['tweetid','timestamp']\n",
    "#                            ) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "Processing /home/twh/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe/sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Processing /home/twh/.cache/pip/wheels/72/93/36/3c7c74a6f2127e71810a0e0f535955175556a434aec55de679/hdbscan-0.8.33-cp38-cp38-linux_x86_64.whl\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/twh/.local/lib/python3.8/site-packages (from bertopic) (5.18.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /home/twh/.local/lib/python3.8/site-packages (from bertopic) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/twh/.local/lib/python3.8/site-packages (from bertopic) (4.66.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /home/twh/.local/lib/python3.8/site-packages (from bertopic) (0.5.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/twh/.local/lib/python3.8/site-packages (from bertopic) (2.0.3)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-2.1.0-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/twh/.local/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.19.0)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.16.0-cp38-cp38-manylinux1_x86_64.whl (6.9 MB)\n",
      "Requirement already satisfied: nltk in /home/twh/.local/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
      "Requirement already satisfied: scipy in /home/twh/.local/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /home/twh/.local/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.99)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "Collecting cython<3,>=0.27\n",
      "  Using cached Cython-0.29.36-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/twh/.local/lib/python3.8/site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/twh/.local/lib/python3.8/site-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from plotly>=4.7.0->bertopic) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/twh/.local/lib/python3.8/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/twh/.local/lib/python3.8/site-packages (from umap-learn>=0.5.0->bertopic) (0.58.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/twh/.local/lib/python3.8/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
      "Requirement already satisfied: tbb>=2019.0 in /home/twh/.local/lib/python3.8/site-packages (from umap-learn>=0.5.0->bertopic) (2021.10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/twh/.local/lib/python3.8/site-packages (from pandas>=1.1.5->bertopic) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/twh/.local/lib/python3.8/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/twh/.local/lib/python3.8/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Requirement already satisfied: networkx in /home/twh/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
      "Collecting nvidia-nccl-cu12==2.18.1; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions in /home/twh/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.8.0)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting triton==2.1.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /home/twh/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (10.3.2.106)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Requirement already satisfied: fsspec in /home/twh/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2023.10.0)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Requirement already satisfied: filelock in /home/twh/.local/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (5.3.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/twh/.local/lib/python3.8/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.5.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/twh/.local/lib/python3.8/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (2023.10.3)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Using cached tokenizers-0.14.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->plotly>=4.7.0->bertopic) (3.0.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /home/twh/.local/lib/python3.8/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (6.8.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /home/twh/.local/lib/python3.8/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.41.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.14.0)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/twh/.local/lib/python3.8/site-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.51.2->umap-learn>=0.5.0->bertopic) (3.17.0)\n",
      "\u001b[31mERROR: tokenizers 0.14.1 has requirement huggingface_hub<0.18,>=0.16.4, but you'll have huggingface-hub 0.19.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: nvidia-nvtx-cu12, mpmath, sympy, nvidia-cublas-cu12, nvidia-cudnn-cu12, nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, triton, nvidia-cuda-nvrtc-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-cupti-cu12, torch, torchvision, tokenizers, safetensors, transformers, sentence-transformers, cython, hdbscan, bertopic\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 28] No space left on device: '/tmp/pip-unpacked-wheel-zj6zkwck/nvidia/cudnn/lib/libcudnn_ops_infer.so.8' -> '/home/twh/.local/lib/python3.8/site-packages/nvidia/cudnn/lib/libcudnn_ops_infer.so.8'\n",
      "\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"cache\" - maybe you meant \"check\"\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
